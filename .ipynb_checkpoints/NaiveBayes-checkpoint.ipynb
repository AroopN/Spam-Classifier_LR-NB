{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from math import log\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopwords = True\n",
    "stopwords=set()\n",
    "\n",
    "if remove_stopwords:\n",
    "    with open(\"stopwords.txt\",'r') as a:\n",
    "        stopwords = set(a.read().split())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def laplace_smoothening(w,d,n):\n",
    "    for i in w:\n",
    "        if i not in d:\n",
    "            d[i]=n\n",
    "        else:\n",
    "            d[i]+=n\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnames(mypath):\n",
    "    from os import walk\n",
    "    f=[]\n",
    "    for (dirpath, dirnames, filenames) in walk(mypath):\n",
    "        f.extend(filenames)\n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(mypath):\n",
    "    \n",
    "    sentences =\"\"\n",
    "    f = fnames(mypath)\n",
    "    prior=len(f)\n",
    "    for i in f:\n",
    "        fin = codecs.open(mypath+i, 'r', encoding='utf-8',\n",
    "                          errors='ignore')\n",
    "        data=fin.read().lower() + \" \"\n",
    "        sentences+=data\n",
    "    return prior,sentences.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    \n",
    "    spams, spam_words = read_data(\"./train/spam/\")\n",
    "    hams, ham_words = read_data(\"./train/ham/\")\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        spam_words = [ x for x in spam_words if x not in stopwords]\n",
    "        ham_words = [ x for x in ham_words if x not in stopwords]\n",
    "    \n",
    "    spam_word_counts = Counter(spam_words)\n",
    "    ham_word_counts = Counter(ham_words)\n",
    "    spam_prior = spams/(spams+hams)\n",
    "    ham_prior = hams/(spams+hams)\n",
    "    vocab = set(spam_words + ham_words)\n",
    "    \n",
    "    #Laplace smoothening\n",
    "    spam_word_counts = laplace_smoothening(vocab, spam_word_counts, 1)\n",
    "    ham_word_counts = laplace_smoothening(vocab, ham_word_counts, 1)\n",
    "\n",
    "    tot_spam_words = sum(spam_word_counts.values())\n",
    "    spam_word_prob = spam_word_counts\n",
    "    for i in vocab:\n",
    "        spam_word_prob[i] = spam_word_prob[i]/tot_spam_words\n",
    "\n",
    "    tot_ham_words = sum(ham_word_counts.values())\n",
    "    ham_word_prob = ham_word_counts\n",
    "    for i in vocab:\n",
    "        ham_word_prob[i] = ham_word_prob[i]/tot_ham_words\n",
    "\n",
    "    return spam_prior, ham_prior, spam_word_prob, ham_word_prob, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10330\n",
      "0.9435146443514645\n"
     ]
    }
   ],
   "source": [
    "spam_prior, ham_prior, spam_word_prob, ham_word_prob, vocab = get_dataset()\n",
    "mypath = \"./test/spam/\"\n",
    "\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "fl=1\n",
    "paths = [\"./test/spam/\"  , \"./test/ham/\"]\n",
    "for mypath in paths:\n",
    "    f = fnames(mypath)\n",
    "    for i in f:\n",
    "        fin = codecs.open(mypath+i, 'r', encoding='utf-8',\n",
    "                        errors='ignore')\n",
    "        sen = fin.read().lower().split()\n",
    "        X.append(sen)\n",
    "        y.append(fl)\n",
    "    fl-=1\n",
    "\n",
    "pred = []\n",
    "print(len(vocab))\n",
    "\n",
    "for i in X:\n",
    "    sp = log(spam_prior)\n",
    "    hp = log(ham_prior)\n",
    "    for j in i:\n",
    "        if j in vocab:\n",
    "            sp += log(spam_word_prob[j])\n",
    "            hp += log(ham_word_prob[j])\n",
    "    if sp > hp:\n",
    "        pred.append(1)\n",
    "    else:\n",
    "        pred.append(0)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(pred, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
